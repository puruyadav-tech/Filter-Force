# -*- coding: utf-8 -*-
"""fraud_app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Xa9IT3yQfQa6fl9GZlMPAGdHdPaz4Qu8
"""


import streamlit as st
import pandas as pd
import numpy as np
import requests
import io
from sklearn.feature_extraction.text import TfidfVectorizer
from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_recall_curve
from imblearn.over_sampling import SMOTE
from scipy.sparse import hstack
import matplotlib.pyplot as plt

st.set_page_config(page_title="Fraud Job Detector", layout="wide")

@st.cache_data
def load_data():
    # Google Drive file ID for training data
    file_id = "1oxygynnHOAU3NU3S8xHRdBS9WA_K1E7y"
    url = f"https://drive.google.com/uc?id={file_id}"

    try:
        response = requests.get(url)
        response.raise_for_status()
        train_df = pd.read_csv(io.BytesIO(response.content))
    except Exception as e:
        st.error(f"Error loading training data from Google Drive: {e}")
        train_df = pd.DataFrame()  # Empty DataFrame fallback

    # Load test data from local file (make sure Test data.csv is uploaded)
    try:
        test_df = pd.read_csv("Test data.csv")
    except Exception as e:
        st.error(f"Error loading test data file: {e}")
        test_df = pd.DataFrame()  # Empty DataFrame fallback

    return train_df, test_df

@st.cache_data
def prepare(df):
    df = df.copy()

    # Ensure expected text columns exist
    for col in ['title', 'description', 'requirements', 'company_profile']:
        if col not in df.columns:
            df[col] = ''
        else:
            df[col] = df[col].fillna('')

    # Handle 'email' column
    if 'email' in df.columns:
        df['email'] = df['email'].fillna('')
    else:
        df['email'] = [''] * len(df)

    # Combine text fields
    df['text'] = (
        df['title'] + ' ' +
        df['description'] + ' ' +
        df['requirements'] + ' ' +
        df['company_profile']
    )

    # Feature engineering
    df['desc_len'] = df['description'].apply(len)
    df['word_count'] = df['description'].apply(lambda x: len(x.split()))
    df['num_digits_in_title'] = df['title'].apply(lambda x: sum(c.isdigit() for c in x))
    df['has_profile'] = (df['company_profile'] != '').astype(int)

    # Flag suspicious words
    suspicious_words = ['money', 'wire', 'bitcoin', 'transfer', 'click']
    df['suspicious_terms'] = df['description'].apply(
        lambda x: int(any(term in x.lower() for term in suspicious_words))
    )

    # Email domain feature
    df['email_domain'] = df['email'].apply(lambda x: x.split('@')[-1] if '@' in x else '')
    df['free_email'] = df['email_domain'].isin(['gmail.com', 'yahoo.com', 'hotmail.com']).astype(int)

    return df


@st.cache_resource
def train_model(train_df):
    if train_df.empty:
        st.warning("Training data not loaded. Model training skipped.")
        return None, None, 0.5

    X = train_df[['text', 'desc_len', 'word_count', 'num_digits_in_title', 'has_profile', 'suspicious_terms', 'free_email']]
    y = train_df['fraudulent']
    tfidf = TfidfVectorizer(max_features=5000, stop_words='english')
    X_tfidf = tfidf.fit_transform(X['text'])
    X_combined = hstack([X_tfidf, X.drop(columns='text').values])
    X_train, X_val, y_train, y_val = train_test_split(X_combined, y, test_size=0.2, stratify=y, random_state=42)
    X_res, y_res = SMOTE(random_state=42).fit_resample(X_train, y_train)
    model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)
    model.fit(X_res, y_res)
    val_probs = model.predict_proba(X_val)[:, 1]
    p, r, thresholds = precision_recall_curve(y_val, val_probs)
    f1s = 2 * p * r / (p + r + 1e-6)
    best_threshold = thresholds[np.argmax(f1s)]
    return model, tfidf, best_threshold


# Load and prepare data
train_df, test_df = load_data()

if not train_df.empty:
    train_df = prepare(train_df)
if not test_df.empty:
    test_df = prepare(test_df)

# Train model only if training data is available
model, tfidf, default_threshold = train_model(train_df)

if not test_df.empty and model is not None and tfidf is not None:
    X_test = test_df[['text', 'desc_len', 'word_count', 'num_digits_in_title', 'has_profile', 'suspicious_terms', 'free_email']]
    X_test_tfidf = tfidf.transform(X_test['text'])
    X_test_combined = hstack([X_test_tfidf, X_test.drop(columns='text').values])
    test_df['fraud_probability'] = model.predict_proba(X_test_combined)[:, 1]
    test_df['fraud_predicted'] = (test_df['fraud_probability'] >= default_threshold).astype(int)
else:
    test_df['fraud_probability'] = []
    test_df['fraud_predicted'] = []

# Sidebar Controls
st.sidebar.title("âš™ï¸ Filters")
threshold = st.sidebar.slider("Fraud Probability Threshold", 0.0, 1.0, float(default_threshold), step=0.01)
search_term = st.sidebar.text_input("ğŸ” Search by Job Title")
high_risk_only = st.sidebar.checkbox("ğŸš© Show High-Risk Only")

# Header
st.markdown("<h1 style='text-align: center; color: #0077b6;'>ğŸ•µï¸â€â™‚ï¸ Job Fraud Detection App</h1>", unsafe_allow_html=True)
st.markdown("<p style='text-align: center;'>Analyze and identify potentially fraudulent job postings using machine learning</p>", unsafe_allow_html=True)
st.markdown("---")

if test_df.empty:
    st.warning("Test data not loaded. Cannot show predictions.")
else:
    # Filter logic
    filtered_df = test_df.copy()
    filtered_df['fraud_predicted'] = (filtered_df['fraud_probability'] >= threshold).astype(int)

    if search_term:
        filtered_df = filtered_df[filtered_df['title'].str.contains(search_term, case=False)]

    if high_risk_only:
        filtered_df = filtered_df[filtered_df['fraud_predicted'] == 1]

    # Metrics
    col1, col2, col3 = st.columns(3)
    col1.metric("ğŸ” Total Jobs", f"{len(test_df)}")
    col2.metric("âš ï¸ Predicted Fraud", f"{filtered_df['fraud_predicted'].sum()}")
    col3.metric("ğŸ“ˆ Threshold Used", f"{threshold:.2f}")

    st.markdown("### ğŸ“‹ Predictions Table")
    st.dataframe(
        filtered_df[['title', 'location', 'fraud_probability', 'fraud_predicted']].sort_values(by='fraud_probability', ascending=False),
        use_container_width=True
    )

    # Download Button
    st.download_button(
        "ğŸ“¥ Download Results as CSV",
        data=filtered_df.to_csv(index=False).encode(),
        file_name="fraud_predictions.csv",
        mime="text/csv"
    )

    # Charts
    st.markdown("### ğŸ“Š Probability Distribution")
    fig, ax = plt.subplots()
    ax.hist(test_df['fraud_probability'], bins=20, color='#0077b6', edgecolor='white')
    ax.set_xlabel("Fraud Probability")
    ax.set_ylabel("Job Count")
    st.pyplot(fig)

    st.markdown("### ğŸ§® Fraud Prediction Breakdown")
    fraud_counts = filtered_df['fraud_predicted'].value_counts()
    labels = ['Not Fraud', 'Fraud']
    sizes = [fraud_counts.get(0, 0), fraud_counts.get(1, 0)]

    fig2, ax2 = plt.subplots()
    ax2.pie(sizes, labels=labels, autopct='%1.1f%%', colors=['#90e0ef', '#ff6b6b'], startangle=90)
    ax2.axis('equal')
    st.pyplot(fig2)

st.markdown("---")
st.markdown("<small>Developed with â¤ï¸ using Streamlit</small>", unsafe_allow_html=True)
