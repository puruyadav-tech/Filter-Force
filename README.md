# fraud-detector-app
üîç Job Fraud Detector | Anveshan Hackathon Project An ML-powered Streamlit web app that detects fake job listings using natural language processing, feature engineering, and XGBoost. Includes real-time predictions, SHAP explanations, search, filtering, and CSV export ‚Äî built to protect job seekers from online scams
https://drive.google.com/file/d/1oxygynnHOAU3NU3S8xHRdBS9WA_K1E7y/view?usp=sharing

 Fraud Detector: Smart Job Scam Detection System
üß† Project Overview
Online job platforms are frequently targeted by scammers. These fake job listings waste applicants‚Äô time and put their personal information at risk.

This project builds a machine learning pipeline that automatically detects fraudulent job listings and presents insights in a Streamlit-powered interactive dashboard.

üéØ Problem Statement
Develop a system to classify job listings as fraudulent or genuine using structured and unstructured data. Display predictions and insights in an intuitive dashboard.

üöÄ Automatic Job Fraud Detection
Classifies job listings as real or fake using advanced machine learning techniques.

üß† Hybrid Feature Engineering
Combines TF-IDF text vectors with structured numeric features:

-Description length & word count

-Suspicious keywords (e.g., money, bitcoin)

-Free email domain indicators (e.g., gmail.com, yahoo.com)

-Digit count in job titles

üå≤ XGBoost Classifier for Accuracy & Speed

Leverages the power of XGBoost ‚Äî a high-performance gradient boosting algorithm ‚Äî for fast training and accurate predictions.

‚öñÔ∏è Class Imbalance Handling with SMOTE
Uses SMOTE oversampling to improve detection of rare fraudulent cases without overfitting.

üß™ F1-Optimized Threshold Tuning

Dynamically selects the best threshold using precision-recall curve to maximize the F1-score ‚Äî ideal for imbalanced data.

üìä Visual Analytics Dashboard

-Histogram of fraud probabilities

-Pie chart of predicted real vs. fake listings

-Threshold vs. F1-score visualization

-SHAP summary plots for feature importance

üîç Explainable AI with SHAP

-Visualize top contributing features

-Interpret individual predictions

-Enhance transparency and trust in model decisions

‚úâÔ∏è High-Risk Job Alert System

Automatically flags listings with fraud probability ‚â• 0.8 and supports email alerts via SMTP integration.

üîó Unified Text + Numeric Feature Pipeline

Combines TF-IDF matrix with custom features using scipy.sparse.hstack for seamless model input.

‚ö° Deployment-Ready with Streamlit

Integrated with a user-friendly Streamlit interface for real-time fraud prediction and insights.

üõ†Ô∏è Technologies Used


| Area          | Tools/Libraries                      |
| ------------- | ------------------------------------ |
| Data Handling | `pandas`, `numpy`                    |
| Modeling      | `XGBoost`, `SMOTE`                   |
| NLP           | `TfidfVectorizer`                    |
| Evaluation    | `F1-score`, `precision-recall` curve |
| Visualization | `matplotlib`, `seaborn`, `SHAP`      |
| Web Dashboard | `Streamlit`                          |
| Deployment    | `Streamlit Cloud`                    |


üßæ Setup Instructions

![image](https://github.com/user-attachments/assets/76357276-f33e-4ec6-abcf-42a88bea3ca9)

‚Ä¢ pandas and numpy: Used for handling structured data and numerical operations.

‚Ä¢ matplotlib.pyplot and seaborn: Used for plotting graphs and visualizing distributions and model results.

‚Ä¢ TfidfVectorizer: Converts text into numerical features based on frequency and importance.

‚Ä¢ RandomForestClassifier: (Though unused here) A machine learning model. You used XGBClassifier instead.

‚Ä¢ XGBClassifier: An optimized version of gradient-boosted trees, very powerful for tabular data.

‚Ä¢ train_test_split: Splits data into training and validation sets.

‚Ä¢ classification_report, confusion_matrix, f1_score, etc.: Used to evaluate model performance.

‚Ä¢ SMOTE: Balances the dataset by generating synthetic examples for the minority class (fraudulent jobs).

‚Ä¢ shap: Helps explain the model‚Äôs predictions and shows which features are most important.

‚Ä¢ warnings.filterwarnings("ignore"): Suppresses warnings for clean output.

![image](https://github.com/user-attachments/assets/bbaa63e7-e7f2-48cf-b056-39a4b1bdac1c)

‚Ä¢ Reading CSV files into pandas DataFrames.

‚Ä¢ on_bad_lines='skip' ensures any corrupted lines or format errors are skipped rather than crashing the program.

![image](https://github.com/user-attachments/assets/729bac06-9b45-4aa4-90cf-bc0602c47fcd)

üßπ Dataset Preparation & Feature Engineering
To ensure robust and consistent model training, we preprocess and enrich the dataset through a series of thoughtful transformations:

1. Safe Copy of the Data
We begin by copying the input DataFrame to avoid modifying the original data. This is a best practice for all preprocessing pipelines.

2. Validate and Clean Text Columns
We ensure key textual columns (title, description, requirements, company_profile) exist. If missing, they're created and filled with empty strings to avoid errors during vectorization.

3. Clean the Email Column
If the email field exists, we replace all missing values with blank strings to maintain formatting and prevent errors during feature extraction.

4. Merge Text for TF-IDF
We combine all main text fields into a unified full_text column, which is used to generate TF-IDF features. This leverages the entire job listing content for better text-based learning.

5. Generate Custom Features
‚Ä¢ We engineer several insightful features to detect fraud patterns:

‚Ä¢ desc_len: Length of the job description (in characters).

‚Ä¢ word_count: Number of words in the description.

‚Ä¢ num_digits_in_title: Count of numerical digits in the job title (e.g., ‚ÄúEarn $5000‚Äù).

‚Ä¢ has_profile: Boolean flag ‚Äî does the company provide a profile?

‚Ä¢ suspicious_terms: Checks for keywords like "bitcoin", "transfer", "click", "money", etc., common in scams.

6. Email-Based Features
email_domain: Extracted from the email (e.g., gmail.com, company.com).

free_email: Flags whether the domain belongs to a free provider (like gmail, yahoo, etc.), often used in fake listings.

7.  Consistency Across Datasets
We apply the same transformation function to both training and test datasets. This guarantees consistent features during model training and inference.


![image](https://github.com/user-attachments/assets/a56f1f07-2b84-4faa-984f-5c485883da3a)

1. Split the dataset:

‚Ä¢ X: Input features from the training data.

‚Ä¢ y: Output labels (0 = Real, 1 = Fraudulent).

‚Ä¢ X_test: Input features from the test data (labels are not available).

2. Features in X:

‚Ä¢ Text feature: Combined text column (formed by concatenating title, description, requirements, company_profile).

‚Ä¢ Handcrafted numeric features:

‚Ä¢ desc_len: Number of characters in the job description.

‚Ä¢ word_count: Number of words in the description.

‚Ä¢ num_digits_in_title: Number of digits in the job title.

‚Ä¢ has_profile: Binary flag indicating presence of a company profile.

‚Ä¢ suspicious_terms: Binary flag if scammy terms like money, bitcoin exist.

‚Ä¢ free_email: Binary flag for free email domains (e.g., gmail.com).

3.  TF-IDF Vectorization

‚Ä¢ To handle text data, we use TF-IDF (Term Frequency-Inverse Document Frequency) ‚Äî a statistical method to evaluate how important a word is in a document relative to the entire corpus 

4. Configuration:

‚Ä¢ max_features = 5000: Use the 5000 most informative words.

‚Ä¢ stop_words = 'english': Removes common unimportant words (e.g., the, and, is).

5. Apply TF-IDF:

‚Ä¢ On X['text'] ‚Üí creates X_tfidf (sparse matrix).

‚Ä¢ On X_test['text'] ‚Üí creates X_test_tfidf (using the same vocabulary as training).

6. Combining All Features

The TF-IDF matrix (text features),

With 6 handcrafted numerical features (from feature engineering).

We use scipy.sparse.hstack() to horizontally stack these two sets of features into one big feature matri.

7. Final matrices:

‚Ä¢ X_combined: Final feature set for training.

‚Ä¢ X_test_combined: Final feature set for test predictions.

This combined matrix is now ready to feed into machine learning models like XGBoost  giving the model both:
These combined matrices are then used to train ML models like XGBoost or Random Forest, benefiting from:

‚Ä¢ Rich semantic content (from text)

‚Ä¢ Strong rule-based signals (from numeric features

![image](https://github.com/user-attachments/assets/1ecbd0f9-25ef-4c36-b796-01609c9bc1d2)

Handling Class Imbalance and Model Training

1. Problem: Class Imbalance

     ‚Ä¢ Fraudulent job postings are rare.

     ‚Ä¢ This causes class imbalance ‚Äî a major challenge in machine learning.

     ‚Ä¢ A model that always predicts ‚Äúnot fraud‚Äù may achieve high accuracy but is useless.

2. Solution: SMOTE (Synthetic Minority Oversampling Technique)

 ‚Ä¢ SMOTE creates synthetic fraud examples by interpolating between real ones.

 ‚Ä¢ This balances the dataset and helps the model learn fraud patterns more effectively.

3. Train-Validation Split

 ‚Ä¢ Dataset is split using train_test_split() with stratify=y to maintain class distribution.

4. Model: XGBoost Classifier

 ‚Ä¢ Chosen for its speed and accuracy in classification tasks.

 ‚Ä¢ Key hyperparameters:

     ‚Ä¢ n_estimators=200: Number of trees.

     ‚Ä¢ max_depth=6: Controls model complexity to avoid overfitting.

     ‚Ä¢ subsample & colsample_bytree: Randomly sample rows and features to enhance generalization.

     ‚Ä¢ eval_metric='logloss': Metric to evaluate binary classification error.

5. Training

     ‚Ä¢ The model is trained on SMOTE-balanced data: X_res, y_res.

 ![image](https://github.com/user-attachments/assets/f9454534-08d6-4d88-82a0-f31798dac96c)

‚Ä¢ Threshold Optimization for Better Performance

1. Default Threshold Issue

  ‚Ä¢ Most models use a default threshold of 0.5 for binary classification.

  ‚Ä¢ On imbalanced datasets, this may not yield optimal results.

2. Threshold Tuning

   ‚Ä¢ Use precision_recall_curve to compute scores across all possible thresholds.

   ‚Ä¢ Calculate F1 Score (harmonic mean of precision and recall) for each threshold.

   ‚Ä¢ Select the best_threshold that maximizes F1 Score.

3. Plot F1 vs Threshold

‚Ä¢ Helps visualize how F1 score changes with different classification thresholds.

‚Ä¢ Aids in selecting the optimal best_threshold.

4. Final Predictions with Best Threshold

‚Ä¢ Apply best_threshold to the model's output probabilities.

‚Ä¢ Generate predictions for evaluation.

5. Evaluate with classification_report()

‚Ä¢ Provides detailed performance metrics:

    ‚Ä¢ Precision: Of all predicted frauds, how many were actually fraud?

    ‚Ä¢ Recall: Of all actual frauds, how many did the model detect?

    ‚Ä¢ F1 Score: Harmonic mean of precision and recall ‚Äî balances false positives and false negatives.

    ‚Ä¢ Support: Number of true instances in each class.

6. Evaluate with confusion_matrix()

  ‚Ä¢ Breaks down predictions into four categories:

        ‚Ä¢ True Positives (TP) ‚Äì Correctly predicted fraud cases.

        ‚Ä¢ True Negatives (TN) ‚Äì Correctly predicted genuine jobs.

        ‚Ä¢ False Positives (FP) ‚Äì Genuine jobs incorrectly labeled as fraud.

        ‚Ä¢ False Negatives (FN) ‚Äì Fraud cases the model failed to detect
        
‚Ä¢ This helps us understand model performance beyond accuracy and ensures it catches fraud with minimal false alarms.


 







